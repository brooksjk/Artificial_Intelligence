{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96903e61-ae25-43eb-a0a3-ddcd7e159d3e",
   "metadata": {},
   "source": [
    "# Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644a194-0ca9-466a-b6e5-74dfcde9eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "maze_rows, maze_cols = 5, 6\n",
    "rewards = np.zeros((maze_rows, maze_cols))\n",
    "rewards[2, 0] = 100  # reward for (1,3)\n",
    "rewards[2, 5] = 100  # reward for (6,3)\n",
    "rewards[0, 1] = -50  # penalty for (2,1)\n",
    "\n",
    "# terminal states\n",
    "terminal_states = [(2, 0), (2, 5)]\n",
    "\n",
    "# actions\n",
    "actions = ['West', 'East', 'North', 'South']\n",
    "action_effects = {\n",
    "    'West': (0, -1),\n",
    "    'East': (0, 1),\n",
    "    'North': (1, 0),\n",
    "    'South': (-1, 0)\n",
    "}\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "Q_values = np.zeros((maze_rows, maze_cols, len(actions)))\n",
    "\n",
    "def q_value_iterations(Q_values, rewards, iterations):\n",
    "    for iteration in range(1, iterations + 1):\n",
    "        print(f\"Iteration {iteration}:\")\n",
    "        new_Q_values = np.copy(Q_values)\n",
    "        for i in range(maze_rows):\n",
    "            for j in range(maze_cols):\n",
    "                if (i, j) in terminal_states:\n",
    "                    for a, action in enumerate(actions):\n",
    "                        new_Q_values[i, j, a] = rewards[i, j]\n",
    "                        print(f\"Q(({j + 1},{i + 1}),{action})={new_Q_values[i, j, a]:.2f}, \", end='')\n",
    "                else:\n",
    "                    for a, action in enumerate(actions):\n",
    "                        next_i = i + action_effects[action][0]\n",
    "                        next_j = j + action_effects[action][1]\n",
    "                        if 0 <= next_i < maze_rows and 0 <= next_j < maze_cols:\n",
    "                            new_Q_values[i, j, a] = rewards[i, j] - 1 + gamma * np.max(Q_values[next_i, next_j])\n",
    "                        else:\n",
    "                            new_Q_values[i, j, a] = rewards[i, j] - 1 + gamma * np.max(Q_values[i, j])\n",
    "                        print(f\"Q(({j + 1},{i + 1}),{action})={new_Q_values[i, j, a]:.2f}, \", end='')\n",
    "                print()\n",
    "        Q_values = new_Q_values\n",
    "        print(\"===================================\")\n",
    "    return Q_values\n",
    "\n",
    "Q_values = q_value_iterations(Q_values, rewards, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff5f54f-c167-4c19-bc46-f34b8275d754",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a95ba7-7305-46cc-a4cc-a96b19e545e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "maze_rows, maze_cols = 5, 6\n",
    "rewards = np.zeros((maze_rows, maze_cols))\n",
    "rewards[2, 0] = 100  # Reward for state (1,3)\n",
    "rewards[2, 5] = 100  # Reward for state (6,3)\n",
    "rewards[0, 1] = -50  # Penalty for state (2,1)\n",
    "terminal_states = [(2, 0), (2, 5)]\n",
    "\n",
    "# actions\n",
    "actions = ['West', 'East', 'North', 'South']\n",
    "action_effects = {\n",
    "    'West': (0, -1),\n",
    "    'East': (0, 1),\n",
    "    'North': (1, 0),\n",
    "    'South': (-1, 0)\n",
    "}\n",
    "\n",
    "#  action probabilities \n",
    "action_probabilities = {\n",
    "    'West': {'West': 1.0},\n",
    "    'East': {'East': 0.4, 'Stay': 0.1, 'West': 0.5},\n",
    "    'North': {'North': 0.8, 'Stay': 0.1, 'West': 0.1},\n",
    "    'South': {'South': 0.8, 'Stay': 0.1, 'West': 0.1}\n",
    "}\n",
    "\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "Q_values = np.zeros((maze_rows, maze_cols, len(actions)))\n",
    "\n",
    "def q_value_iterations_probabilistic(Q_values, rewards, iterations):\n",
    "    for iteration in range(1, iterations + 1):\n",
    "        print(f\"Iteration {iteration}:\")\n",
    "        new_Q_values = np.copy(Q_values)\n",
    "        for i in range(maze_rows):\n",
    "            for j in range(maze_cols):\n",
    "                if (i, j) in terminal_states:\n",
    "                    for a, action in enumerate(actions):\n",
    "                        new_Q_values[i, j, a] = rewards[i, j]\n",
    "                        print(f\"Q(({j + 1},{i + 1}),{action})={new_Q_values[i, j, a]:.2f}, \", end='')\n",
    "                else:\n",
    "                    for a, action in enumerate(actions):\n",
    "                        expected_value = 0\n",
    "                        for next_action, prob in action_probabilities[action].items():\n",
    "                            if next_action == 'Stay':\n",
    "                                next_i, next_j = i, j\n",
    "                            else:\n",
    "                                next_i = i + action_effects[next_action][0]\n",
    "                                next_j = j + action_effects[next_action][1]\n",
    "                            if 0 <= next_i < maze_rows and 0 <= next_j < maze_cols:\n",
    "                                expected_value += prob * (rewards[i, j] - 1 + gamma * np.max(Q_values[next_i, next_j]))\n",
    "                            else:\n",
    "                                expected_value += prob * (rewards[i, j] - 1 + gamma * np.max(Q_values[i, j]))\n",
    "                        new_Q_values[i, j, a] = expected_value\n",
    "                        print(f\"Q(({j + 1},{i + 1}),{action})={new_Q_values[i, j, a]:.2f}, \", end='')\n",
    "                print()\n",
    "        Q_values = new_Q_values\n",
    "        print(\"===================================\")\n",
    "    return Q_values\n",
    "\n",
    "Q_values_probabilistic = q_value_iterations_probabilistic(Q_values, rewards, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9148ea01-c517-4137-ad1b-1a71e8ab9f5f",
   "metadata": {},
   "source": [
    "# Answer to questions "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c207f4ea-1342-4023-9d4f-e6b0f93385ae",
   "metadata": {},
   "source": [
    "1. Convergence iteration: 11\n",
    "\n",
    "2. In the second iteration of Q-value updates for the probabilistic model, the states that are updated are those that are directly affected by the terminal states or the penalty state due to the reward propagation through the state space.\n",
    "\n",
    "This includes: (1,1), (1,2), (1,4), (3,1), (6,2), and (6,4)\n",
    "\n",
    "3. \n",
    "Best action for state (3,5) in the deterministic model: West\n",
    "Best action for state (4,5) in the deterministic model: East\n",
    "3. Best action for state (3,5) in the probabilistic model: South\n",
    "3. Best action for state (4,5) in the probabilistic model: South"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
